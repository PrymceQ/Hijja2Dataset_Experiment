{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/硕士相关/福瑞/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2ecb3e1fa51e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 加载数据将每一个数据变形成32x32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'D:/硕士相关/福瑞/train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'D:/硕士相关/福瑞/test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PythonAnaconda\\newmain\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PythonAnaconda\\newmain\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PythonAnaconda\\newmain\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PythonAnaconda\\newmain\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PythonAnaconda\\newmain\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PythonAnaconda\\newmain\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/硕士相关/福瑞/train.csv'"
     ]
    }
   ],
   "source": [
    "# 加载包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 加载数据将每一个数据变形成32x32\n",
    "train_data = pd.read_csv(r'D:/硕士相关/福瑞/train.csv', header=0)\n",
    "test_data = pd.read_csv(r'D:/硕士相关/福瑞/test.csv', header=0)\n",
    "\n",
    "# 对label进行映射替换\n",
    "keys = list(np.unique(train_data[['Secondary folder']]))\n",
    "values = list(range(len(keys)))\n",
    "dic = dict(zip(keys, values))\n",
    "print(dic)\n",
    "\n",
    "# 进行label替换\n",
    "train_data['Secondary folder'] = train_data.iloc[:, 3:4].replace(dic)\n",
    "test_data['Secondary folder'] = test_data.iloc[:, 3:4].replace(dic)\n",
    "\n",
    "X_train = np.array(train_data.iloc[:, -1024:])\n",
    "y_train = np.array(train_data['Secondary folder']).astype(int)\n",
    "X_test = np.array(test_data.iloc[:, -1024:])\n",
    "y_test = np.array(test_data['Secondary folder']).astype(int)\n",
    "\n",
    "\n",
    "X_test = torch.from_numpy(X_test).reshape(len(X_test), 1, 32, 32)\n",
    "y_test = torch.from_numpy(y_test).squeeze()\n",
    "X_train = torch.from_numpy(X_train).reshape(len(X_train), 1, 32, 32)\n",
    "y_train = torch.from_numpy(y_train).squeeze()\n",
    "\n",
    "print('X_test: ', X_test.shape,\n",
    "      '\\ny_test: ', y_test.shape,\n",
    "      '\\nX_train: ', X_train.shape,\n",
    "      '\\ny_train: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 训练函数 #############\n",
    "def train(net, train_features, train_labels, val_features, val_labels, num_epochs, \n",
    "          batch_size, optimizer, loss_function, def_init_weight=None, device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")): \n",
    "    \"\"\"PARAMS:\n",
    "    net: 需要进行训练的网络结构\n",
    "    train_features: 训练features数据\n",
    "    train_labels: 训练targets数据\n",
    "    val_features: 验证features数据\n",
    "    val_labels: 验证targets数据\n",
    "    num_epochs: 需要训练的轮次\n",
    "    batch_size: batch_size\n",
    "    optimizer: 定义的优化器(torch.optim)\n",
    "    loss_function: 自定义的损失函数(torch.nn)\n",
    "    def_init_weight: 模型参数初始化(默认为None, torch.nn.init)\n",
    "                        def init_weight(m):\n",
    "                            if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "                            nn.init.xavier_uniform_(m.weight)\n",
    "    device: 自动搜索设备进行运行\n",
    "                        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if def_init_weight != None:\n",
    "        net.apply(def_init_weight)\n",
    "        # net = torch.load('./Net_model.pt') # 使用预训练的pt文件\n",
    "   \n",
    "        \n",
    "    dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    train_iter = DataLoader(dataset, batch_size, shuffle=True) \n",
    "    \n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss_a=[]\n",
    "        \n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output  = net(X.to(torch.float32))\n",
    "            loss = loss_function(output, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_a.append(loss)\n",
    "\n",
    "        if epoch % 30 == 0:\n",
    "            print('loss_train: ', sum(loss_a)/len(train_iter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_train:  tensor(4.6975, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss_train:  tensor(4.6827, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss_train:  tensor(4.6827, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss_train:  tensor(4.6827, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss_train:  tensor(4.6826, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss_train:  tensor(4.6825, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "loss_train:  tensor(4.6826, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def init_weight(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "class Reshape(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 1, 32, 32)\n",
    "Net_model = nn.Sequential(\n",
    "    Reshape(),\n",
    "    nn.Conv2d(1, 64, 3, padding=1), \n",
    "    nn.ReLU(inplace=True),\n",
    "    \n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2d(64, 128, 3, padding=1), \n",
    "    nn.ReLU(inplace=True),\n",
    "    \n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2d(128, 256, 3, padding=1), \n",
    "    nn.ReLU(inplace=True),\n",
    "    \n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 4 * 4, 1024), \n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(1024, 512), \n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(512, 108))\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "optimizer = torch.optim.SGD(Net_model.parameters(), lr = 0.5)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "train(net=Net_model, train_features=X_train, train_labels=y_train, val_features=X_test, val_labels=y_test, num_epochs=num_epochs, \n",
    "          batch_size=batch_size, optimizer=optimizer, loss_function=loss_function, def_init_weight=init_weight, device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "torch.save(Net_model, 'Net_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用test数据进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cross Entropy Loss:  4.6809 \n",
      " Accuracy:  1.1107300213703402 %\n",
      " Cross Entropy Loss:  4.6812 \n",
      " Accuracy:  1.1121603189591858 %\n"
     ]
    }
   ],
   "source": [
    "net_test = torch.load('./Net_model.pt').to(device=torch.device('cpu'))\n",
    "def evalution(X, y):\n",
    "      test_pred = net_test(X.to(torch.float32))\n",
    "      result = torch.max(test_pred, 1)[1].view(y.size())\n",
    "      corrects = (result.data == y.data).sum().item()\n",
    "      accuracy = corrects*100.0/len(X)\n",
    "      loss_func = nn.CrossEntropyLoss()\n",
    "      loss = loss_func(test_pred, y.long()).data.numpy()\n",
    "\n",
    "      print(' Cross Entropy Loss: ', round(float(loss), 4), '\\n',\n",
    "            'Accuracy: ', accuracy, '%')\n",
    "      \n",
    "evalution(X_train, y_train)\n",
    "evalution(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4012971ec14703d4f4fb623ef66de65c80f37e7105b36eebf5e5977c8c8cb53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
